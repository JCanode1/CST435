{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CST-435 Recurrent Neural Network Project\n",
    "\n",
    "## Team: \n",
    "Gabriel Aracena, Aaron Galicia, Joshua Canode\n",
    "\n",
    "## Project Description\n",
    "\n",
    "This assignment accomplishes two goals. It demonstrates how neural networks can be used in forecasting and how they can be used in practical applications involving text (e.g., completing a search request on Google).\n",
    "\n",
    "Using a large set of texts for training, build an RNN that suggests the next word in a sentence (sequential learning). Consider the entire sentence when completing the sentence instead of words by themselves.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The Goal of this project is to create a Recurrent Neural Network (RNN) model that can predict the next word in a given sentence, thereby enabling the completion of sentences in a coherent and contextually relevant manner. The primary goal is to demonstrate the capabilities of neural networks in the field of text prediction, particularly for applications like search engine queries, chatbots, or predictive text input systems.\n",
    "    \n",
    "## Data\n",
    "\n",
    "We chose to use a synthetic dataset generated by ChatGPT. The data contains a variety of short sentences. To introduce greater variety and enhance the diversity of the dataset, these sentences are categorized into questions, first-person perspectives, third-person perspectives, and different types of statements.\n",
    "\n",
    "\n",
    "## Algorithm of the solution \n",
    "\n",
    "In the development of our RNN model, we followed a structured approach encompassing several pivotal phases:\n",
    "\n",
    "1. **Data Collection and Preprocessing**: This involved gathering and preparing the dataset, including punctuation removal, tokenization, and integer encoding of words.\n",
    "\n",
    "2. **Many-to-One Sequence Mapping**: We formulated the problem as a many-to-one sequence mapping task, focusing on predicting the next word in a given sequence.\n",
    "\n",
    "3. **Model Construction with Keras**: The RNN model was built using Keras, featuring multiple layers:\n",
    "    - An embedding layer for word representations\n",
    "    - A masking layer to handle words without embeddings\n",
    "    - An LSTM layer to capture sequential dependencies\n",
    "    - Dense layers for feature enhancement\n",
    "    - An output layer for predicting the next word\n",
    "    - Integration of pretrained GloVe word embeddings.\n",
    "\n",
    "4. **Embedding Quality Assessment**: We evaluated the quality of these embeddings by analyzing cosine similarities between word vectors.\n",
    "\n",
    "5. **Training with Model Checkpoint and Early Stopping**: To optimize training, we implemented techniques such as Model Checkpoint and Early Stopping.\n",
    "\n",
    "6. **Text Generation Function**: A function for generating text predictions was developed, allowing the model to complete sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Analysis of the findings  \n",
    "\n",
    "From manual evaluation, it is claer to see that the results are fairly good. The grammar and the sentance structure of the senctances are quite good.\n",
    "\n",
    "with this input text, \"*it's not just about winning; it's about the*\" we got sentance completions such as:\n",
    "- *But remember, it's not just about winning; it's about the new challenges as opportunities to grow and learn.*\n",
    "- *But remember, it's not just about winning; it's about the friends we make along the way.*\n",
    "And as expected with a higher temperature the reusulting sentances appear to be more interesting and creative, but make a little less sense. \n",
    "- *But remember, it's not just about winning; it's about the force her a jedi that how be that legend of the silence comes questions?*\n",
    "\n",
    "\n",
    "The text generation results at various temperature settings demonstrate the impact of temperature on the diversity and coherence of generated text. Lower temperatures produce highly deterministic text closely tied to the input, while higher temperatures lead to more creative but potentially less coherent outputs. The choice of temperature depends on the desired balance between maintaining context and introducing novelty, making it a critical parameter for controlling the behavior of text generation models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Read the source text from a file\n",
    "file_path = 'source_text.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    source_text = file.read()\n",
    "\n",
    "# Custom tokenizer that does not filter out punctuation (except quotes and double quotes)\n",
    "tokenizer = Tokenizer(filters='\"#$%&()*+-/:;<=>@[\\\\]^_{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts([source_text])\n",
    "sequence = tokenizer.texts_to_sequences([source_text])[0]\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Generate input sequences for training\n",
    "input_sequences = [sequence[:i] for i in range(1, len(sequence))]\n",
    "\n",
    "# Calculate average sequence length before padding and adjust it if needed\n",
    "average_sequence_len = np.mean([len(seq) for seq in input_sequences])\n",
    "max_sequence_len = int(average_sequence_len * 1.5)\n",
    "print(f\"Average sequence length: {average_sequence_len}, Max sequence length after padding: {max_sequence_len}\")\n",
    "\n",
    "# Pad sequences to the same length\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "target_word = to_categorical(sequence[1:], num_classes=total_words)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embedding_index = {}\n",
    "glove_path = 'GloVe840B/glove.840B.300d.txt'\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        try:\n",
    "            # Ensure that conversion to float is possible\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            word = values[0]\n",
    "            embedding_index[word] = coefs\n",
    "        except ValueError:\n",
    "            # Skip the problematic line\n",
    "            continue\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_dim = 300  # GloVe vector sizen\n",
    "embedding_matrix = np.zeros((total_words, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, embedding_dim, weights=[embedding_matrix], input_length=max_sequence_len, trainable=False))\n",
    "model.add(GRU(units=100, return_sequences=False))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 128  # Adjust as needed\n",
    "\n",
    "# Define Early Stopping callback\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Masking, Dropout\n",
    "\n",
    "# Assuming total_words, max_sequence_len, and embedding_matrix are already defined\n",
    "\n",
    "# Model definition with LSTM\n",
    "lstm_model = Sequential()\n",
    "\n",
    "# The Embedding layer with pretrained GloVe weights, set to non-trainable\n",
    "lstm_model.add(Embedding(input_dim=total_words, \n",
    "                         output_dim=300, \n",
    "                         weights=[embedding_matrix], \n",
    "                         input_length=max_sequence_len, \n",
    "                         trainable=False))\n",
    "\n",
    "# The Masking layer to ignore the all-zero input (i.e., padding)\n",
    "lstm_model.add(Masking(mask_value=0.0))\n",
    "\n",
    "# The LSTM layer with dropout\n",
    "lstm_model.add(LSTM(units=100, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n",
    "\n",
    "# A Dense layer with relu activation\n",
    "lstm_model.add(Dense(units=100, activation='relu'))\n",
    "\n",
    "# A Dropout layer to prevent overfitting\n",
    "lstm_model.add(Dropout(0.2))\n",
    "\n",
    "# The final Dense layer with a softmax activation function\n",
    "lstm_model.add(Dense(units=total_words, activation='softmax'))\n",
    "\n",
    "# Compile the model with the Adam optimizer\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Display the model's architecture\n",
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sequence length: 1071.5, Max sequence length after padding: 1607\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - 66s 4s/step - loss: 6.8532\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 68s 4s/step - loss: 6.3052\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 69s 4s/step - loss: 6.0308\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 73s 4s/step - loss: 5.9319\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 76s 4s/step - loss: 5.8362\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 76s 4s/step - loss: 5.7232\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 71s 4s/step - loss: 5.5943\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 75s 4s/step - loss: 5.4601\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 74s 4s/step - loss: 5.3223\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 86s 5s/step - loss: 5.1866\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 83s 5s/step - loss: 5.0479\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 90s 5s/step - loss: 4.9105\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 89s 5s/step - loss: 4.7731\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 83s 5s/step - loss: 4.6369\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 76s 4s/step - loss: 4.4988\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 81s 5s/step - loss: 4.3640\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 88s 5s/step - loss: 4.2285\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 85s 5s/step - loss: 4.0958\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 79s 5s/step - loss: 3.9636\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 87s 5s/step - loss: 3.8257\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 83s 5s/step - loss: 3.6959\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 82s 5s/step - loss: 3.5639\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 98s 6s/step - loss: 3.4351\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 93s 5s/step - loss: 3.3083\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 92s 5s/step - loss: 3.1853\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 95s 6s/step - loss: 3.0608\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 96s 6s/step - loss: 2.9415\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 93s 5s/step - loss: 2.8240\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 98s 6s/step - loss: 2.7082\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 85s 5s/step - loss: 2.5962\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 79s 5s/step - loss: 2.4881\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 88s 5s/step - loss: 2.3904\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 83s 5s/step - loss: 2.2818\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 76s 4s/step - loss: 2.1835\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 77s 5s/step - loss: 2.0889\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 88s 5s/step - loss: 1.9982\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 93s 5s/step - loss: 1.9115\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 78s 5s/step - loss: 1.8272\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 71s 4s/step - loss: 1.7444\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 71s 4s/step - loss: 1.6664\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 77s 5s/step - loss: 1.5911\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 79s 5s/step - loss: 1.5212\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 77s 5s/step - loss: 1.4518\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 72s 4s/step - loss: 1.3871\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 85s 5s/step - loss: 1.3245\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 79s 5s/step - loss: 1.2634\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 72s 4s/step - loss: 1.2052\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 75s 4s/step - loss: 1.1494\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 77s 4s/step - loss: 1.1043\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 73s 4s/step - loss: 1.0549\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 83s 5s/step - loss: 1.0030\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 81s 5s/step - loss: 0.9537\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 92s 5s/step - loss: 0.9095\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 82s 5s/step - loss: 0.8678\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 97s 6s/step - loss: 0.8274\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 93s 5s/step - loss: 0.7871\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 100s 6s/step - loss: 0.7521\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 98s 6s/step - loss: 0.7176\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 91s 5s/step - loss: 0.6823\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 142s 8s/step - loss: 0.6521\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 140s 8s/step - loss: 0.6239\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 161s 9s/step - loss: 0.5946\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 132s 8s/step - loss: 0.5656\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 129s 8s/step - loss: 0.5404\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 135s 8s/step - loss: 0.5144\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 139s 8s/step - loss: 0.4918\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 126s 7s/step - loss: 0.4700\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 125s 7s/step - loss: 0.4477\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 138s 8s/step - loss: 0.4269\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 125s 7s/step - loss: 0.4081\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 125s 7s/step - loss: 0.3895\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 131s 8s/step - loss: 0.3729\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 123s 7s/step - loss: 0.3572\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 126s 7s/step - loss: 0.3410\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 150s 9s/step - loss: 0.3260\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 95s 5s/step - loss: 0.3124\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 75s 4s/step - loss: 0.2985\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 76s 4s/step - loss: 0.2866\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 86s 5s/step - loss: 0.2754\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 103s 6s/step - loss: 0.2641\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 143s 9s/step - loss: 0.2539\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 109s 6s/step - loss: 0.2433\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 77s 5s/step - loss: 0.2348\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 96s 6s/step - loss: 0.2249\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 129s 8s/step - loss: 0.2161\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 149s 9s/step - loss: 0.2079\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 155s 9s/step - loss: 0.2003\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 86s 5s/step - loss: 0.1932\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 72s 4s/step - loss: 0.1858\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 71s 4s/step - loss: 0.1791\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 74s 4s/step - loss: 0.1730\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 79s 5s/step - loss: 0.1664\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 86s 5s/step - loss: 0.1609\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 86s 5s/step - loss: 0.1556\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 86s 5s/step - loss: 0.1501\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 86s 5s/step - loss: 0.1453\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 86s 5s/step - loss: 0.1406\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 85s 5s/step - loss: 0.1360\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 86s 5s/step - loss: 0.1316\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 83s 5s/step - loss: 0.1275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x13ae28565d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(input_sequences, target_word, epochs=100, verbose=1, callbacks=[early_stopping], batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model ###\n",
    "This should be ran every time a model is trained and is tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtlar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models\\model_20231105-195652.h5\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "base_dir = 'models'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "# After training, create a timestamp or a unique identifier for the model\n",
    "model_id = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f\"model_{model_id}.h5\"\n",
    "model_path = os.path.join(base_dir, model_name)\n",
    "\n",
    "# Save the model to the specified directory\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Later, to load the model, you can use:\n",
    "# model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Next Word ###\n",
    "This just generates the next immediate word. Number of words generated can be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are the\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # Convert predictions to probabilities\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    preds = np.log(preds + 1e-7) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    \n",
    "    # Normalize predictions\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    # Sample a single prediction with the probabilities to return a likely next word index\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text_seq(model, tokenizer, text, num_words, max_sequence_len, temperature=1.0):\n",
    "    input_text = text\n",
    "    for _ in range(num_words):\n",
    "        # Convert the input text to a sequence of word indexes\n",
    "        input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
    "\n",
    "        # Pad the sequence to the required length\n",
    "        input_seq = pad_sequences([input_seq], maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "        # Predict the next word index\n",
    "        predictions = model.predict(input_seq, verbose=0)[0]\n",
    "        predicted_word_index = sample(predictions, temperature)\n",
    "\n",
    "        # Convert the index to a word\n",
    "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "\n",
    "        # Append the predicted word to the input text\n",
    "        input_text += ' ' + predicted_word\n",
    "\n",
    "    return input_text.strip()\n",
    "\n",
    "# Test the model on a new input sequence with temperature\n",
    "test_text = \"Hello, how are\"\n",
    "num_words = 1\n",
    "temperature = 0.75  # Adjust the temperature as needed to vary randomness\n",
    "generated_text = generate_text_seq(model, tokenizer, test_text, num_words, max_sequence_len, temperature)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Rest of Sentence ###\n",
    "Generates words until a punctuation is found. There is a max word limit to prevent a feedback loop. \n",
    "\n",
    "If words a getting in a feedback loop or you are not happy with results, try adjusting the temperature before training again. Higher values gives more randomness while lower values has a higher likely hood to have a feedback loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input text: But remember, it's not just about winning; it's about the\n",
      "\n",
      "Generating with temperature 0.1:\n",
      "But remember, it's not just about winning; it's about the new challenges as opportunities to grow and learn. \n",
      "\n",
      "Generating with temperature 0.2:\n",
      "But remember, it's not just about winning; it's about the new student at hogwarts high? \n",
      "\n",
      "Generating with temperature 0.5:\n",
      "But remember, it's not just about winning; it's about the new challenges as opportunities to grow and learn. \n",
      "\n",
      "Generating with temperature 0.7:\n",
      "But remember, it's not just about winning; it's about the friends we make along the way. \n",
      "\n",
      "Generating with temperature 0.8:\n",
      "But remember, it's not just about winning; it's about the friends we make along the way. \n",
      "\n",
      "Generating with temperature 0.85:\n",
      "But remember, it's not just about winning; it's about the friends we make along the way. \n",
      "\n",
      "Generating with temperature 0.9:\n",
      "But remember, it's not just about winning; it's about the new challenges as opportunities to each your way. \n",
      "\n",
      "Generating with temperature 1.2:\n",
      "But remember, it's not just about winning; it's about the lightsaber? \n",
      "\n",
      "Generating with temperature 2:\n",
      "But remember, it's not just about winning; it's about the force her a jedi that how be that legend of the silence comes questions? \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-7) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text_seq(model, tokenizer, text, max_sequence_len, temperature=1.0, punctuations=\".!?\"):\n",
    "    input_text = text\n",
    "    word_count = 0\n",
    "    max_words = 50\n",
    "    print(input_text, end=' ')\n",
    "    while True:\n",
    "        word_count += 1\n",
    "        if word_count > max_words:\n",
    "            print(\"\\nError: Max Words Reached Before Punctuation\")\n",
    "            break\n",
    "        # Convert the input text to a sequence of word indexes\n",
    "        input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
    "\n",
    "        # Pad the sequence to the required length\n",
    "        input_seq = pad_sequences([input_seq], maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "        # Predict the next word index\n",
    "        predictions = model.predict(input_seq, verbose=0)[0]\n",
    "        predicted_word_index = sample(predictions, temperature)\n",
    "\n",
    "        # Convert the index to a word\n",
    "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "\n",
    "        # Append the predicted word to the input text and print it\n",
    "        input_text += ' ' + predicted_word\n",
    "        print(predicted_word, end=' ', flush=True)\n",
    "\n",
    "        # Break if the predicted word ends with a punctuation mark or is empty\n",
    "        if any(predicted_word.endswith(punct) for punct in punctuations) or predicted_word == '':\n",
    "            break\n",
    "\n",
    "    print()  # To ensure we move to a new line after the sentence ends\n",
    "    return input_text.strip()\n",
    "\n",
    "# Define a list of temperatures. For example, low=0.2, medium=0.7, high=1.2\n",
    "temperatures = [0.1 , 0.2, 0.5 , 0.7, 0.8 , 0.85 , 0.9, 1.2, 2]\n",
    "\n",
    "# Test the model on a new input sequence with different temperatures\n",
    "test_text = \"But remember, it's not just about winning; it's about the\"\n",
    "print(\"The input text:\", test_text)\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nGenerating with temperature {temp}:\")\n",
    "    generated_text = generate_text_seq(model, tokenizer, test_text, max_sequence_len, temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
