{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Needs To Be Done ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sequence length: 2019.5, Max sequence length after padding: 3029\n",
      "Embedding dimensions: 50, GRU units: 45\n",
      "Batch size: 20\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Nadam\n",
    "import os\n",
    "\n",
    "# Read the source text from a file\n",
    "file_path = 'source_text.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    source_text = file.read()\n",
    "\n",
    "# Custom tokenizer that does not filter out punctuation (except quotes and double quotes)\n",
    "tokenizer = Tokenizer(filters='\"#$%&()*+-/:;<=>@[\\\\]^_{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts([source_text])\n",
    "sequence = tokenizer.texts_to_sequences([source_text])[0]\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Generate input sequences for training\n",
    "input_sequences = [sequence[:i] for i in range(1, len(sequence))]\n",
    "\n",
    "# Calculate average sequence length before padding and adjust it if needed\n",
    "average_sequence_len = np.mean([len(seq) for seq in input_sequences])\n",
    "max_sequence_len = int(average_sequence_len * 1.5)\n",
    "print(f\"Average sequence length: {average_sequence_len}, Max sequence length after padding: {max_sequence_len}\")\n",
    "\n",
    "# Pad sequences to the same length\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "target_word = to_categorical(sequence[1:], num_classes=total_words)\n",
    "\n",
    "# Adjust the model parameters\n",
    "min_threshold = 10\n",
    "embedding_dim = max(min_threshold, min(50, total_words // 20))\n",
    "gru_units = max(min_threshold, min(50, total_words // 40))\n",
    "print(f\"Embedding dimensions: {embedding_dim}, GRU units: {gru_units}\")\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, embedding_dim, input_length=max_sequence_len))\n",
    "model.add(GRU(units=gru_units, return_sequences=False))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Compile the model with Nadam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Nadam())\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 20\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Define Early Stopping callback with increased patience\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Create directory for saving checkpoints\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# High initial learning rate\n",
    "initial_learning_rate = 0.1  # This is a dramatic increase from the default\n",
    "\n",
    "# Compile the model with Nadam optimizer and a higher learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Nadam(learning_rate=initial_learning_rate))\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=2, min_lr=0.0001, verbose=1)\n",
    "\n",
    "# Create ModelCheckpoint callback to save the full model\n",
    "checkpoint_filepath = checkpoint_dir + '/checkpoint-epoch-{epoch:02d}-loss-{loss:.2f}.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=False,\n",
    "    save_freq='epoch',\n",
    "    period=5,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sequence length: 2019.5, Max sequence length after padding: 3029\n",
      "Embedding dimensions: 50, GRU units: 45\n",
      "Batch size: 20\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/100\n",
      "202/202 [==============================] - 304s 1s/step - loss: 12.9640 - lr: 0.1000\n",
      "Epoch 2/100\n",
      "202/202 [==============================] - 307s 2s/step - loss: 17.2681 - lr: 0.1000\n",
      "Epoch 3/100\n",
      "202/202 [==============================] - ETA: 0s - loss: 18.0375\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
      "202/202 [==============================] - 361s 2s/step - loss: 18.0375 - lr: 0.1000\n",
      "Epoch 4/100\n",
      "202/202 [==============================] - 338s 2s/step - loss: 10.7250 - lr: 0.0200\n",
      "Epoch 5/100\n",
      "202/202 [==============================] - ETA: 0s - loss: 6.3919\n",
      "Epoch 5: saving model to ./training_checkpoints\\checkpoint-epoch-05-loss-6.39.h5\n",
      "202/202 [==============================] - 299s 1s/step - loss: 6.3919 - lr: 0.0200\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaron\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 328s 2s/step - loss: 4.3321 - lr: 0.0200\n",
      "Epoch 7/100\n",
      "202/202 [==============================] - 317s 2s/step - loss: 3.2168 - lr: 0.0200\n",
      "Epoch 8/100\n",
      "202/202 [==============================] - 304s 2s/step - loss: 2.6074 - lr: 0.0200\n",
      "Epoch 9/100\n",
      "202/202 [==============================] - 314s 2s/step - loss: 2.2156 - lr: 0.0200\n",
      "Epoch 10/100\n",
      "202/202 [==============================] - ETA: 0s - loss: 1.9971\n",
      "Epoch 10: saving model to ./training_checkpoints\\checkpoint-epoch-10-loss-2.00.h5\n",
      "202/202 [==============================] - 326s 2s/step - loss: 1.9971 - lr: 0.0200\n",
      "Epoch 11/100\n",
      "155/202 [======================>.......] - ETA: 1:17 - loss: 1.8495"
     ]
    }
   ],
   "source": [
    "# Train the model with the new callbacks\n",
    "model.fit(\n",
    "    input_sequences,\n",
    "    target_word,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, checkpoint_callback, reduce_lr],\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# After training, save the final model\n",
    "model.save(checkpoint_dir + '/final_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model ###\n",
    "This should be ran every time a model is trained and is tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models\\model_20231104-165145.h5\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "base_dir = 'models'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "# After training, create a timestamp or a unique identifier for the model\n",
    "model_id = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f\"model_{model_id}.h5\"\n",
    "model_path = os.path.join(base_dir, model_name)\n",
    "\n",
    "# Save the model to the specified directory\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Later, to load the model, you can use:\n",
    "# model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# The path to the directory where checkpoints are saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# The exact filename of the checkpoint file\n",
    "checkpoint_filepath = checkpoint_dir + '/final_model.h5'\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model(checkpoint_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Next Word ###\n",
    "This just generates the next immediate word. Number of words generated can be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He fixed the guitar,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # Convert predictions to probabilities\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    preds = np.log(preds + 1e-7) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    \n",
    "    # Normalize predictions\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    # Sample a single prediction with the probabilities to return a likely next word index\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text_seq(model, tokenizer, text, num_words, max_sequence_len, temperature=1.0):\n",
    "    input_text = text\n",
    "    for _ in range(num_words):\n",
    "        # Convert the input text to a sequence of word indexes\n",
    "        input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
    "\n",
    "        # Pad the sequence to the required length\n",
    "        input_seq = pad_sequences([input_seq], maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "        # Predict the next word index\n",
    "        predictions = model.predict(input_seq, verbose=0)[0]\n",
    "        predicted_word_index = sample(predictions, temperature)\n",
    "\n",
    "        # Convert the index to a word\n",
    "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "\n",
    "        # Append the predicted word to the input text\n",
    "        input_text += ' ' + predicted_word\n",
    "\n",
    "    return input_text.strip()\n",
    "\n",
    "# Test the model on a new input sequence with temperature\n",
    "test_text = \"He fixed the\"\n",
    "num_words = 1\n",
    "temperature = 0.75  # Adjust the temperature as needed to vary randomness\n",
    "generated_text = generate_text_seq(model, tokenizer, test_text, num_words, max_sequence_len, temperature)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Rest of Sentence ###\n",
    "Generates words until a punctuation is found. There is a max word limit to prevent a feedback loop. \n",
    "\n",
    "If words a getting in a feedback loop or you are not happy with results, try adjusting the temperature before training again. Higher values gives more randomness while lower values has a higher likely hood to have a feedback loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input text: I am the\n",
      "\n",
      "Generating with temperature 0.1:\n",
      "I am the leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves \n",
      "Error: Max Words Reached Before Punctuation\n",
      "\n",
      "\n",
      "Generating with temperature 0.2:\n",
      "I am the leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves \n",
      "Error: Max Words Reached Before Punctuation\n",
      "\n",
      "\n",
      "Generating with temperature 0.5:\n",
      "I am the leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves leaves plans through the where stars let's its this let's the and quest with thanksgiving, present was wondering at the world at night. \n",
      "\n",
      "Generating with temperature 0.7:\n",
      "I am the plans journey take? \n",
      "\n",
      "Generating with temperature 0.8:\n",
      "I am the leaves leaves the has one? \n",
      "\n",
      "Generating with temperature 0.85:\n",
      "I am the force new student forgot daily twilight laughter. \n",
      "\n",
      "Generating with temperature 0.9:\n",
      "I am the leaves look away. \n",
      "\n",
      "Generating with temperature 1.2:\n",
      "I am the dead. \n",
      "\n",
      "Generating with temperature 2:\n",
      "I am the dead. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-7) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text_seq(model, tokenizer, text, max_sequence_len, temperature=1.0, punctuations=\".!?\"):\n",
    "    input_text = text\n",
    "    word_count = 0\n",
    "    max_words = 50\n",
    "    print(input_text, end=' ')\n",
    "    while True:\n",
    "        word_count += 1\n",
    "        if word_count > max_words:\n",
    "            print(\"\\nError: Max Words Reached Before Punctuation\")\n",
    "            break\n",
    "        # Convert the input text to a sequence of word indexes\n",
    "        input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
    "\n",
    "        # Pad the sequence to the required length\n",
    "        input_seq = pad_sequences([input_seq], maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "        # Predict the next word index\n",
    "        predictions = model.predict(input_seq, verbose=0)[0]\n",
    "        predicted_word_index = sample(predictions, temperature)\n",
    "\n",
    "        # Convert the index to a word\n",
    "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "\n",
    "        # Append the predicted word to the input text and print it\n",
    "        input_text += ' ' + predicted_word\n",
    "        print(predicted_word, end=' ', flush=True)\n",
    "\n",
    "        # Break if the predicted word ends with a punctuation mark or is empty\n",
    "        if any(predicted_word.endswith(punct) for punct in punctuations) or predicted_word == '':\n",
    "            break\n",
    "\n",
    "    print()  # To ensure we move to a new line after the sentence ends\n",
    "    return input_text.strip()\n",
    "\n",
    "# Define a list of temperatures. For example, low=0.2, medium=0.7, high=1.2\n",
    "temperatures = [0.1 , 0.2, 0.5 , 0.7, 0.8 , 0.85 , 0.9, 1.2, 2]\n",
    "\n",
    "# Test the model on a new input sequence with different temperatures\n",
    "test_text = \"I am the\"\n",
    "print(\"The input text:\", test_text)\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nGenerating with temperature {temp}:\")\n",
    "    generated_text = generate_text_seq(model, tokenizer, test_text, max_sequence_len, temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes ###\n",
    "From Aaron, Not Chat GPT Generated Lol:\n",
    "\n",
    "I believe the training takes the entire training set as one big context instead of training on each sentence as a single context. This is probably why it generates long senteces that don't make much sense. We can change how the training is tokenized to fix this but I don't think its a big deal for the scope of this project. We can just add this thought to the conclusion. The project says to use the embedddings from the Glove algorithm, this is not implemented yet nor do I know if Artsi actually wants us to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
