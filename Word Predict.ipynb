{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Needs To Be Done ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sequence length: 1071.5, Max sequence length after padding: 1607\n",
      "Embedding dimensions: 50, GRU units: 25\n",
      "Batch size: 103\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - 17s 660ms/step - loss: 6.9328\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - 14s 651ms/step - loss: 6.8941\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - 14s 681ms/step - loss: 6.6239\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - 15s 693ms/step - loss: 6.1959\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - 15s 696ms/step - loss: 6.0575\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - 15s 736ms/step - loss: 6.0223\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - 15s 717ms/step - loss: 6.0139\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - 14s 670ms/step - loss: 6.0106\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - 14s 670ms/step - loss: 6.0068\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - 14s 667ms/step - loss: 5.9991\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - 14s 664ms/step - loss: 5.9808\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - 15s 706ms/step - loss: 5.9459\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - 16s 744ms/step - loss: 5.8888\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - 14s 652ms/step - loss: 5.8295\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - 14s 674ms/step - loss: 5.7709\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - 14s 653ms/step - loss: 5.7195\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - 14s 650ms/step - loss: 5.6750\n",
      "Epoch 18/100\n",
      "21/21 [==============================] - 14s 684ms/step - loss: 5.6358\n",
      "Epoch 19/100\n",
      "21/21 [==============================] - 14s 681ms/step - loss: 5.6006\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - 14s 651ms/step - loss: 5.5679\n",
      "Epoch 21/100\n",
      "21/21 [==============================] - 14s 652ms/step - loss: 5.5370\n",
      "Epoch 22/100\n",
      "21/21 [==============================] - 16s 742ms/step - loss: 5.5063\n",
      "Epoch 23/100\n",
      "21/21 [==============================] - 14s 677ms/step - loss: 5.4770\n",
      "Epoch 24/100\n",
      "21/21 [==============================] - 17s 821ms/step - loss: 5.4475\n",
      "Epoch 25/100\n",
      "21/21 [==============================] - 15s 715ms/step - loss: 5.4165\n",
      "Epoch 26/100\n",
      "21/21 [==============================] - 16s 780ms/step - loss: 5.3860\n",
      "Epoch 27/100\n",
      "21/21 [==============================] - 17s 792ms/step - loss: 5.3542\n",
      "Epoch 28/100\n",
      "21/21 [==============================] - 14s 679ms/step - loss: 5.3209\n",
      "Epoch 29/100\n",
      "21/21 [==============================] - 14s 677ms/step - loss: 5.2873\n",
      "Epoch 30/100\n",
      "21/21 [==============================] - 14s 672ms/step - loss: 5.2530\n",
      "Epoch 31/100\n",
      "21/21 [==============================] - 14s 677ms/step - loss: 5.2179\n",
      "Epoch 32/100\n",
      "21/21 [==============================] - 14s 671ms/step - loss: 5.1831\n",
      "Epoch 33/100\n",
      "21/21 [==============================] - 14s 674ms/step - loss: 5.1474\n",
      "Epoch 34/100\n",
      "21/21 [==============================] - 14s 678ms/step - loss: 5.1110\n",
      "Epoch 35/100\n",
      "21/21 [==============================] - 14s 676ms/step - loss: 5.0736\n",
      "Epoch 36/100\n",
      "21/21 [==============================] - 14s 673ms/step - loss: 5.0350\n",
      "Epoch 37/100\n",
      "21/21 [==============================] - 14s 673ms/step - loss: 4.9969\n",
      "Epoch 38/100\n",
      "21/21 [==============================] - 14s 681ms/step - loss: 4.9579\n",
      "Epoch 39/100\n",
      "21/21 [==============================] - 14s 682ms/step - loss: 4.9178\n",
      "Epoch 40/100\n",
      "21/21 [==============================] - 14s 674ms/step - loss: 4.8755\n",
      "Epoch 41/100\n",
      "21/21 [==============================] - 14s 677ms/step - loss: 4.8335\n",
      "Epoch 42/100\n",
      "21/21 [==============================] - 14s 677ms/step - loss: 4.7903\n",
      "Epoch 43/100\n",
      "21/21 [==============================] - 14s 677ms/step - loss: 4.7476\n",
      "Epoch 44/100\n",
      "21/21 [==============================] - 15s 733ms/step - loss: 4.7044\n",
      "Epoch 45/100\n",
      "21/21 [==============================] - 19s 928ms/step - loss: 4.6618\n",
      "Epoch 46/100\n",
      "21/21 [==============================] - 20s 934ms/step - loss: 4.6192\n",
      "Epoch 47/100\n",
      "21/21 [==============================] - 18s 840ms/step - loss: 4.5753\n",
      "Epoch 48/100\n",
      "21/21 [==============================] - 16s 764ms/step - loss: 4.5313\n",
      "Epoch 49/100\n",
      "21/21 [==============================] - 16s 783ms/step - loss: 4.4874\n",
      "Epoch 50/100\n",
      "21/21 [==============================] - 17s 785ms/step - loss: 4.4434\n",
      "Epoch 51/100\n",
      "21/21 [==============================] - 16s 771ms/step - loss: 4.3984\n",
      "Epoch 52/100\n",
      "21/21 [==============================] - 16s 742ms/step - loss: 4.3536\n",
      "Epoch 53/100\n",
      "21/21 [==============================] - 16s 773ms/step - loss: 4.3082\n",
      "Epoch 54/100\n",
      "21/21 [==============================] - 16s 748ms/step - loss: 4.2635\n",
      "Epoch 55/100\n",
      "21/21 [==============================] - 14s 678ms/step - loss: 4.2186\n",
      "Epoch 56/100\n",
      "21/21 [==============================] - 14s 670ms/step - loss: 4.1734\n",
      "Epoch 57/100\n",
      "21/21 [==============================] - 14s 679ms/step - loss: 4.1298\n",
      "Epoch 58/100\n",
      "21/21 [==============================] - 14s 682ms/step - loss: 4.0858\n",
      "Epoch 59/100\n",
      "21/21 [==============================] - 14s 677ms/step - loss: 4.0436\n",
      "Epoch 60/100\n",
      "21/21 [==============================] - 14s 676ms/step - loss: 4.0017\n",
      "Epoch 61/100\n",
      "21/21 [==============================] - 14s 676ms/step - loss: 3.9590\n",
      "Epoch 62/100\n",
      "21/21 [==============================] - 14s 674ms/step - loss: 3.9182\n",
      "Epoch 63/100\n",
      "21/21 [==============================] - 15s 707ms/step - loss: 3.8773\n",
      "Epoch 64/100\n",
      "21/21 [==============================] - 16s 741ms/step - loss: 3.8360\n",
      "Epoch 65/100\n",
      "21/21 [==============================] - 15s 720ms/step - loss: 3.7958\n",
      "Epoch 66/100\n",
      "21/21 [==============================] - 16s 767ms/step - loss: 3.7556\n",
      "Epoch 67/100\n",
      "21/21 [==============================] - 18s 875ms/step - loss: 3.7163\n",
      "Epoch 68/100\n",
      "21/21 [==============================] - 17s 811ms/step - loss: 3.6766\n",
      "Epoch 69/100\n",
      "21/21 [==============================] - 15s 722ms/step - loss: 3.6382\n",
      "Epoch 70/100\n",
      "21/21 [==============================] - 16s 756ms/step - loss: 3.5994\n",
      "Epoch 71/100\n",
      "21/21 [==============================] - 15s 729ms/step - loss: 3.5617\n",
      "Epoch 72/100\n",
      "21/21 [==============================] - 16s 779ms/step - loss: 3.5262\n",
      "Epoch 73/100\n",
      "21/21 [==============================] - 15s 703ms/step - loss: 3.4898\n",
      "Epoch 74/100\n",
      "21/21 [==============================] - 15s 696ms/step - loss: 3.4535\n",
      "Epoch 75/100\n",
      "21/21 [==============================] - 15s 719ms/step - loss: 3.4177\n",
      "Epoch 76/100\n",
      "21/21 [==============================] - 16s 747ms/step - loss: 3.3837\n",
      "Epoch 77/100\n",
      "21/21 [==============================] - 16s 752ms/step - loss: 3.3490\n",
      "Epoch 78/100\n",
      "21/21 [==============================] - 14s 673ms/step - loss: 3.3158\n",
      "Epoch 79/100\n",
      "21/21 [==============================] - 15s 700ms/step - loss: 3.2824\n",
      "Epoch 80/100\n",
      "21/21 [==============================] - 16s 745ms/step - loss: 3.2481\n",
      "Epoch 81/100\n",
      "21/21 [==============================] - 16s 763ms/step - loss: 3.2151\n",
      "Epoch 82/100\n",
      "21/21 [==============================] - 15s 719ms/step - loss: 3.1825\n",
      "Epoch 83/100\n",
      "21/21 [==============================] - 15s 702ms/step - loss: 3.1513\n",
      "Epoch 84/100\n",
      "21/21 [==============================] - 15s 711ms/step - loss: 3.1194\n",
      "Epoch 85/100\n",
      "21/21 [==============================] - 14s 675ms/step - loss: 3.0886\n",
      "Epoch 86/100\n",
      "21/21 [==============================] - 15s 738ms/step - loss: 3.0588\n",
      "Epoch 87/100\n",
      "21/21 [==============================] - 16s 752ms/step - loss: 3.0291\n",
      "Epoch 88/100\n",
      "21/21 [==============================] - 14s 684ms/step - loss: 3.0008\n",
      "Epoch 89/100\n",
      "21/21 [==============================] - 14s 686ms/step - loss: 2.9723\n",
      "Epoch 90/100\n",
      "21/21 [==============================] - 15s 729ms/step - loss: 2.9431\n",
      "Epoch 91/100\n",
      "21/21 [==============================] - 15s 716ms/step - loss: 2.9142\n",
      "Epoch 92/100\n",
      "21/21 [==============================] - 16s 773ms/step - loss: 2.8863\n",
      "Epoch 93/100\n",
      "21/21 [==============================] - 15s 688ms/step - loss: 2.8598\n",
      "Epoch 94/100\n",
      "21/21 [==============================] - 14s 687ms/step - loss: 2.8325\n",
      "Epoch 95/100\n",
      "21/21 [==============================] - 14s 679ms/step - loss: 2.8062\n",
      "Epoch 96/100\n",
      "21/21 [==============================] - 14s 684ms/step - loss: 2.7794\n",
      "Epoch 97/100\n",
      "21/21 [==============================] - 14s 685ms/step - loss: 2.7534\n",
      "Epoch 98/100\n",
      "21/21 [==============================] - 14s 687ms/step - loss: 2.7275\n",
      "Epoch 99/100\n",
      "21/21 [==============================] - 14s 687ms/step - loss: 2.7023\n",
      "Epoch 100/100\n",
      "21/21 [==============================] - 15s 727ms/step - loss: 2.6777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x15322c17010>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Read the source text from a file\n",
    "file_path = 'source_text.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    source_text = file.read()\n",
    "\n",
    "# Custom tokenizer that does not filter out punctuation (except quotes and double quotes)\n",
    "tokenizer = Tokenizer(filters='\"#$%&()*+-/:;<=>@[\\\\]^_{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts([source_text])\n",
    "sequence = tokenizer.texts_to_sequences([source_text])[0]\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Generate input sequences for training\n",
    "input_sequences = [sequence[:i] for i in range(1, len(sequence))]\n",
    "\n",
    "# Calculate average sequence length before padding and adjust it if needed\n",
    "average_sequence_len = np.mean([len(seq) for seq in input_sequences])\n",
    "max_sequence_len = int(average_sequence_len * 1.5)\n",
    "print(f\"Average sequence length: {average_sequence_len}, Max sequence length after padding: {max_sequence_len}\")\n",
    "\n",
    "# Pad sequences to the same length\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "target_word = to_categorical(sequence[1:], num_classes=total_words)\n",
    "\n",
    "# Adjust the model parameters\n",
    "min_threshold = 10\n",
    "embedding_dim = max(min_threshold, min(50, total_words // 20))\n",
    "gru_units = max(min_threshold, min(50, total_words // 40))\n",
    "print(f\"Embedding dimensions: {embedding_dim}, GRU units: {gru_units}\")\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, embedding_dim, input_length=max_sequence_len))\n",
    "model.add(GRU(units=gru_units, return_sequences=False))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = max(32, min(1024, total_words // 10))\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Define Early Stopping callback\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_sequences, target_word, epochs=100, verbose=1, callbacks=[early_stopping], batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model ###\n",
    "This should be ran every time a model is trained and is tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models\\model_20231104-165145.h5\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "base_dir = 'models'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "# After training, create a timestamp or a unique identifier for the model\n",
    "model_id = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f\"model_{model_id}.h5\"\n",
    "model_path = os.path.join(base_dir, model_name)\n",
    "\n",
    "# Save the model to the specified directory\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Later, to load the model, you can use:\n",
    "# model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Next Word ###\n",
    "This just generates the next immediate word. Number of words generated can be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are the\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # Convert predictions to probabilities\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    preds = np.log(preds + 1e-7) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    \n",
    "    # Normalize predictions\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    # Sample a single prediction with the probabilities to return a likely next word index\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text_seq(model, tokenizer, text, num_words, max_sequence_len, temperature=1.0):\n",
    "    input_text = text\n",
    "    for _ in range(num_words):\n",
    "        # Convert the input text to a sequence of word indexes\n",
    "        input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
    "\n",
    "        # Pad the sequence to the required length\n",
    "        input_seq = pad_sequences([input_seq], maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "        # Predict the next word index\n",
    "        predictions = model.predict(input_seq, verbose=0)[0]\n",
    "        predicted_word_index = sample(predictions, temperature)\n",
    "\n",
    "        # Convert the index to a word\n",
    "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "\n",
    "        # Append the predicted word to the input text\n",
    "        input_text += ' ' + predicted_word\n",
    "\n",
    "    return input_text.strip()\n",
    "\n",
    "# Test the model on a new input sequence with temperature\n",
    "test_text = \"Hello, how are\"\n",
    "num_words = 1\n",
    "temperature = 0.75  # Adjust the temperature as needed to vary randomness\n",
    "generated_text = generate_text_seq(model, tokenizer, test_text, num_words, max_sequence_len, temperature)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Rest of Sentence ###\n",
    "Generates words until a punctuation is found. There is a max word limit to prevent a feedback loop. \n",
    "\n",
    "If words a getting in a feedback loop or you are not happy with results, try adjusting the temperature before training again. Higher values gives more randomness while lower values has a higher likely hood to have a feedback loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input text: But remember, it's not just about winning; it's about the\n",
      "\n",
      "Generating with temperature 0.1:\n",
      "But remember, it's not just about winning; it's about the new within as dusk falls. \n",
      "\n",
      "Generating with temperature 0.2:\n",
      "But remember, it's not just about winning; it's about the new language how are the force of the silent ambassador of his blade. \n",
      "\n",
      "Generating with temperature 0.5:\n",
      "But remember, it's not just about winning; it's about the seedlings with a members. \n",
      "\n",
      "Generating with temperature 0.7:\n",
      "But remember, it's not just about winning; it's about the opportunity to the best medicine, my secret is the cop just two guys, facades, saving the floor, a puppies doing with a little worlds. \n",
      "\n",
      "Generating with temperature 0.8:\n",
      "But remember, it's not just about winning; it's about the mouths of graves, than the force of meant capable of an labor challenges on confirm lost guys, hues club. \n",
      "\n",
      "Generating with temperature 0.85:\n",
      "But remember, it's not just about winning; it's about the season's place for i'm a galaxy equilibrium that echoes with a joy in glacial thunderclap. \n",
      "\n",
      "Generating with temperature 0.9:\n",
      "But remember, it's not just about winning; it's about the perspicacious bibliophile pierce the flick of centuries past. \n",
      "\n",
      "Generating with temperature 1.2:\n",
      "But remember, it's not just about winning; it's about the floor, you cascade of companionship and floats her say amidst the companion of worlds favorites? \n",
      "\n",
      "Generating with temperature 2:\n",
      "But remember, it's not just about winning; it's about the garden nestled ball move? \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-7) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text_seq(model, tokenizer, text, max_sequence_len, temperature=1.0, punctuations=\".!?\"):\n",
    "    input_text = text\n",
    "    word_count = 0\n",
    "    max_words = 50\n",
    "    print(input_text, end=' ')\n",
    "    while True:\n",
    "        word_count += 1\n",
    "        if word_count > max_words:\n",
    "            print(\"\\nError: Max Words Reached Before Punctuation\")\n",
    "            break\n",
    "        # Convert the input text to a sequence of word indexes\n",
    "        input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
    "\n",
    "        # Pad the sequence to the required length\n",
    "        input_seq = pad_sequences([input_seq], maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "        # Predict the next word index\n",
    "        predictions = model.predict(input_seq, verbose=0)[0]\n",
    "        predicted_word_index = sample(predictions, temperature)\n",
    "\n",
    "        # Convert the index to a word\n",
    "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "\n",
    "        # Append the predicted word to the input text and print it\n",
    "        input_text += ' ' + predicted_word\n",
    "        print(predicted_word, end=' ', flush=True)\n",
    "\n",
    "        # Break if the predicted word ends with a punctuation mark or is empty\n",
    "        if any(predicted_word.endswith(punct) for punct in punctuations) or predicted_word == '':\n",
    "            break\n",
    "\n",
    "    print()  # To ensure we move to a new line after the sentence ends\n",
    "    return input_text.strip()\n",
    "\n",
    "# Define a list of temperatures. For example, low=0.2, medium=0.7, high=1.2\n",
    "temperatures = [0.1 , 0.2, 0.5 , 0.7, 0.8 , 0.85 , 0.9, 1.2, 2]\n",
    "\n",
    "# Test the model on a new input sequence with different temperatures\n",
    "test_text = \"But remember, it's not just about winning; it's about the\"\n",
    "print(\"The input text:\", test_text)\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nGenerating with temperature {temp}:\")\n",
    "    generated_text = generate_text_seq(model, tokenizer, test_text, max_sequence_len, temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes ###\n",
    "From Aaron, Not Chat GPT Generated Lol:\n",
    "\n",
    "I believe the training takes the entire training set as one big context instead of training on each sentence as a single context. This is probably why it generates long senteces that don't make much sense. We can change how the training is tokenized to fix this but I don't think its a big deal for the scope of this project. We can just add this thought to the conclusion. The project says to use the embedddings from the Glove algorithm, this is not implemented yet nor do I know if Artsi actually wants us to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
