{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Application: Sentimental Analysis on Steam Reviews (Possibly?)\n",
    "\n",
    "## Team\n",
    "\n",
    "* Gabriel Aracena\n",
    "* Joshua Canode\n",
    "* Aaron Galicia\n",
    "\n",
    "### Project Description\n",
    "\n",
    "A key area of knowledge in data analytics is the ability to extract meaning from text. This assignment provides the foundational skills in this area by detecting whether a text conveys a positive or negative message.\n",
    "\n",
    "Analyze the sentiment (e.g., negative, neutral, positive) conveyed in a large body (corpus) of texts using the NLTK package in Python. Complete the steps below. Then, write a comprehensive technical report as a Python Jupyter notebook to include all code, code comments, all outputs, plots, and analysis. Make sure the project documentation contains a) Problem statement, b) Algorithm of the solution, c) Analysis of the findings, and d) References.\n",
    "\n",
    "## Abstract\n",
    "\n",
    "TODO\n",
    "\n",
    "### Data Preparation:\n",
    "\n",
    "TODO\n",
    "\n",
    "### ANN Model Building:\n",
    "\n",
    "TODO\n",
    "\n",
    "\n",
    "### Training the ANN:\n",
    "\n",
    "\n",
    "\n",
    "### Evaluation:\n",
    "\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "\n",
    "## Interpretation and Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   app_id        app_name                                        review_text  \\\n",
      "0      10  Counter-Strike                                    Ruined my life.   \n",
      "1      10  Counter-Strike  This will be more of a ''my experience with th...   \n",
      "2      10  Counter-Strike                      This game saved my virginity.   \n",
      "3      10  Counter-Strike  • Do you like original games? • Do you like ga...   \n",
      "4      10  Counter-Strike           Easy to learn, hard to master.             \n",
      "\n",
      "   review_score  review_votes  \n",
      "0             1             0  \n",
      "1             1             1  \n",
      "2             1             0  \n",
      "3             1             0  \n",
      "4             1             1  \n",
      "         app_id                                   app_name  \\\n",
      "301327    12210  Grand Theft Auto IV: The Complete Edition   \n",
      "1662500  226320                        Marvel Heroes Omega   \n",
      "2061157  236450           PAC-MAN Championship Edition DX+   \n",
      "1171799  218620                                   PAYDAY 2   \n",
      "1450080  221640                              Super Hexagon   \n",
      "\n",
      "                                               review_text  review_score  \\\n",
      "301327   Best bowling simulator 2014 10/10 It has good ...             1   \n",
      "1662500  Marvel characters? Check. Tons of loot? Check....             1   \n",
      "2061157  This game while its not the original is defina...             1   \n",
      "1171799  This game ♥♥♥♥ing awesome ,You can be professi...             1   \n",
      "1450080  If you are high, play this game. 420/420 would...             1   \n",
      "\n",
      "         review_votes  \n",
      "301327              1  \n",
      "1662500             0  \n",
      "2061157             0  \n",
      "1171799             0  \n",
      "1450080             0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         app_id                                   app_name  \\\n",
      "301327    12210  Grand Theft Auto IV: The Complete Edition   \n",
      "1662500  226320                        Marvel Heroes Omega   \n",
      "2061157  236450           PAC-MAN Championship Edition DX+   \n",
      "1171799  218620                                   PAYDAY 2   \n",
      "1450080  221640                              Super Hexagon   \n",
      "\n",
      "                                               review_text  review_score  \\\n",
      "301327   Best bowling simulator 2014 10/10 It has good ...             1   \n",
      "1662500  Marvel characters? Check. Tons of loot? Check....             1   \n",
      "2061157  This game while its not the original is defina...             1   \n",
      "1171799  This game ♥♥♥♥ing awesome ,You can be professi...             1   \n",
      "1450080  If you are high, play this game. 420/420 would...             1   \n",
      "\n",
      "         review_votes  \n",
      "301327              1  \n",
      "1662500             0  \n",
      "2061157             0  \n",
      "1171799             0  \n",
      "1450080             0  \n",
      "(64171, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sampling the dataset to decrease run time\n",
    "sample_size = int(0.01 * len(df))\n",
    "reduced_sample = df.sample(n=sample_size, random_state=42) \n",
    "print(reduced_sample.head())\n",
    "\n",
    "print(reduced_sample.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\josh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\josh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "# Specify the NLTK data path explicitly\n",
    "nltk.data.path.append('C:/Users/josh/nltk_data')  # Replace with the actual path to your nltk_data directory\n",
    "\n",
    "# Download the required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_lower(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "    else:\n",
    "        cleaned_text = \"\"\n",
    "\n",
    "    return text\n",
    "\n",
    "# tokenize the text\n",
    "def tokenize_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        cleaned_text = \" \".join(tokens)\n",
    "    else:\n",
    "        cleaned_text = \"\"\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    if isinstance(text, str):\n",
    "        # Removing Punctuation\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in string.punctuation]\n",
    "        cleaned_text = \" \".join(tokens)\n",
    "    else:\n",
    "        cleaned_text = \"\"\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Stop Word Removal\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        cleaned_text = \" \".join(filtered_tokens)\n",
    "    else:\n",
    "        cleaned_text = \"\"\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "import re\n",
    "# handleing things like 10/10\n",
    "def replace_good_ratings(text):\n",
    "    pattern = r'(\\d+)/(\\d+)'\n",
    "\n",
    "    def replace(match):\n",
    "        numerator = int(match.group(1))\n",
    "        denominator = int(match.group(2))\n",
    "\n",
    "        # Check if the numerator is not 0\n",
    "        if numerator != 0:\n",
    "            return 'great'\n",
    "        else:\n",
    "            return 'very bad'  # Replace \"0/number\" with \"very bad\"\n",
    "\n",
    "    cleaned_text = re.sub(pattern, replace, text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case the text\n",
    "reduced_sample['review_text'] = df['review_text'].apply(preprocess_text_lower)\n",
    "# 6 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "reduced_sample['review_text'] = reduced_sample['review_text'].apply(tokenize_text)\n",
    "# 24 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "reduced_sample['review_text'] = reduced_sample['review_text'].apply(remove_punctuation)\n",
    "# 31 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "reduced_sample['review_text'] = reduced_sample['review_text'].apply(remove_stopwords)\n",
    "# 50 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace good ratings\n",
    "reduced_sample['review_text'] = reduced_sample['review_text'].apply(replace_good_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         app_id                                   app_name  \\\n",
      "301327    12210  Grand Theft Auto IV: The Complete Edition   \n",
      "1662500  226320                        Marvel Heroes Omega   \n",
      "2061157  236450           PAC-MAN Championship Edition DX+   \n",
      "1171799  218620                                   PAYDAY 2   \n",
      "1450080  221640                              Super Hexagon   \n",
      "\n",
      "                                               review_text  review_score  \\\n",
      "301327    best bowling simulator 2014 great good storyline             1   \n",
      "1662500  marvel characters check tons loot check tons c...             1   \n",
      "2061157  game original definately one best renditions p...             1   \n",
      "1171799  game ♥♥♥♥ing awesome professional heister fun ...             1   \n",
      "1450080                    high play game great would dank             1   \n",
      "\n",
      "         review_votes  \n",
      "301327              1  \n",
      "1662500             0  \n",
      "2061157             0  \n",
      "1171799             0  \n",
      "1450080             0  \n"
     ]
    }
   ],
   "source": [
    "# Print the result (original and cleaned text for the first few rows)\n",
    "print(reduced_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64171, 6)\n",
      "         app_id                                   app_name  \\\n",
      "301327    12210  Grand Theft Auto IV: The Complete Edition   \n",
      "1662500  226320                        Marvel Heroes Omega   \n",
      "2061157  236450           PAC-MAN Championship Edition DX+   \n",
      "1171799  218620                                   PAYDAY 2   \n",
      "1450080  221640                              Super Hexagon   \n",
      "\n",
      "                                               review_text  review_score  \\\n",
      "301327    best bowling simulator 2014 great good storyline             1   \n",
      "1662500  marvel characters check tons loot check tons c...             1   \n",
      "2061157  game original definately one best renditions p...             1   \n",
      "1171799  game ♥♥♥♥ing awesome professional heister fun ...             1   \n",
      "1450080                    high play game great would dank             1   \n",
      "\n",
      "         review_votes      sentiment_scores  \n",
      "301327              1  [[[[[0.00046087]]]]]  \n",
      "1662500             0  [[[[[0.00065986]]]]]  \n",
      "2061157             0  [[[[[0.00073476]]]]]  \n",
      "1171799             0  [[[[[0.00059749]]]]]  \n",
      "1450080             0  [[[[[0.00041958]]]]]  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "print(reduced_sample.shape)\n",
    "\n",
    "# 1. TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust the number of features as needed\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(reduced_sample['review_text'])\n",
    "tfidf_matrix = csr_matrix(tfidf_matrix)\n",
    "\n",
    "# 2. Calculate Sentiment Scores in Batches and Append to DataFrame\n",
    "batch_size = 1000  # Number of rows to process in each batch\n",
    "sentiment_scores = []\n",
    "\n",
    "for start in range(0, len(reduced_sample), batch_size):\n",
    "    end = min(start + batch_size, len(reduced_sample))\n",
    "    batch_tfidf_matrix = tfidf_matrix[start:end]\n",
    "    batch_scores = batch_tfidf_matrix.mean(axis=1)\n",
    "    sentiment_scores.extend(batch_scores)\n",
    "\n",
    "# Add the 'sentiment_scores' column to 'reduced_sample' from the TF-IDF scores\n",
    "reduced_sample['sentiment_scores'] = sentiment_scores\n",
    "\n",
    "# Print the result\n",
    "print(reduced_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "def preprocess_text2(text):\n",
    "    if isinstance(text, str):\n",
    "        # Lowercasing\n",
    "        text = text.lower()\n",
    "\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Removing Punctuation\n",
    "        tokens = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "        # Stop Word Removal\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token, pos=\"v\") for token in filtered_tokens]\n",
    "\n",
    "        # Join the filtered and lemmatized tokens back into a single string\n",
    "        cleaned_text = \" \".join(lemmatized_tokens)\n",
    "\n",
    "        # HTML tag removal\n",
    "        soup = BeautifulSoup(cleaned_text, \"html.parser\")\n",
    "        cleaned_text = soup.get_text()\n",
    "\n",
    "        # Replacing emoticons and abbreviations\n",
    "        replacements = {\n",
    "            \":)\": \"smile\",\n",
    "            \"lol\": \"laugh out loud\",\n",
    "            # Add more replacements as needed\n",
    "        }\n",
    "        for key, value in replacements.items():\n",
    "            cleaned_text = cleaned_text.replace(key, value)\n",
    "\n",
    "        # Spell checking\n",
    "        spell = SpellChecker()\n",
    "        words = cleaned_text.split()\n",
    "        corrected_words = [spell.correction(word) for word in words]\n",
    "        cleaned_text = \" \".join(corrected_words)\n",
    "    else:\n",
    "        # Handle missing values (NaN or non-string values)\n",
    "        cleaned_text = \"\"\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the preprocessing function to the 'review_text' column and create a new column 'preprocessed_text2'\n",
    "df['preprocessed_text2'] = df['review_text'].apply(preprocess_text2)\n",
    "\n",
    "# Print the result (original and cleaned text for the first few rows)\n",
    "print(df[['review_text', 'preprocessed_text2']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity Recognition:\n",
      "(S (PERSON James/NNP) is/VBZ from/IN (LOC Intel/NNP))\n",
      "\n",
      "Sentiment Analysis:\n",
      "Sentence:  James loves working at Intel\n",
      "Positive Sentiment:  0.481\n",
      "Negative Sentiment:  0.0\n",
      "Neutral Sentiment:  0.519\n",
      "Compound Sentiment:  0.5719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\josh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.chunk import ChunkParserI\n",
    "from nltk.chunk.util import conlltags2tree, tree2conlltags\n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import nltk\n",
    "nltk.data.path.append('C:\\\\Users\\\\josh/nltk_data')\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "class NEChunkParser(ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t, c) for w, t, c in sent] for sent in train_sents]\n",
    "        self.tagger = BigramTagger(train_data, backoff=UnigramTagger(train_data))\n",
    "        self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag if chunktag is not None else 'O' for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag) in zip(sentence, chunktags)]\n",
    "        return conlltags2tree(conlltags)\n",
    "\n",
    "    def analyze_sentiment(self, sentence):\n",
    "        sentence_text = ' '.join(word for word, pos in sentence)\n",
    "        score = self.sentiment_analyzer.polarity_scores(sentence_text)\n",
    "        return score\n",
    "\n",
    "# Expanded sample training data\n",
    "train_sents = [\n",
    "    [('James', 'NNP', 'B-PERSON'), ('works', 'VBZ', 'O'), ('in', 'IN', 'O'), ('Intel', 'NNP', 'B-ORG')],\n",
    "    [('Mary', 'NNP', 'B-PERSON'), ('lives', 'VBZ', 'O'), ('in', 'IN', 'O'), ('New York', 'NNP', 'B-LOC')],\n",
    "    [('Google', 'NNP', 'B-ORG'), ('is', 'VBZ', 'O'), ('a', 'DT', 'O'), ('technology', 'NN', 'O'), ('company', 'NN', 'O')],\n",
    "    [('Barack', 'NNP', 'B-PERSON'), ('Obama', 'NNP', 'I-PERSON'), ('was', 'VBD', 'O'), ('the', 'DT', 'O'), ('president', 'NN', 'O'), ('of', 'IN', 'O'), ('the', 'DT', 'O'), ('United States', 'NNP', 'B-LOC')],\n",
    "    [('The', 'DT', 'O'), ('Eiffel', 'NNP', 'B-LOC'), ('Tower', 'NNP', 'I-LOC'), ('is', 'VBZ', 'O'), ('in', 'IN', 'O'), ('Paris', 'NNP', 'B-LOC')],\n",
    "    [('Apple', 'NNP', 'B-ORG'), ('produces', 'VBZ', 'O'), ('the', 'DT', 'O'), ('iPhone', 'NN', 'O')]\n",
    "]\n",
    "\n",
    "chunker = NEChunkParser(train_sents)\n",
    "\n",
    "# Test the custom NER\n",
    "test_sent = [('James', 'NNP'), ('is', 'VBZ'), ('from', 'IN'), ('Intel', 'NNP')]\n",
    "parsed_tree = chunker.parse(test_sent)\n",
    "print(\"Named Entity Recognition:\")\n",
    "print(parsed_tree)\n",
    "\n",
    "# Test sentiment analysis\n",
    "test_sentence = [('James', 'NNP'), ('loves', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Intel', 'NNP')]\n",
    "sentiment_score = chunker.analyze_sentiment(test_sentence)\n",
    "print(\"\\nSentiment Analysis:\")\n",
    "print(\"Sentence: \", \" \".join(word for word, pos in test_sentence))\n",
    "print(\"Positive Sentiment: \", sentiment_score['pos'])\n",
    "print(\"Negative Sentiment: \", sentiment_score['neg'])\n",
    "print(\"Neutral Sentiment: \", sentiment_score['neu'])\n",
    "print(\"Compound Sentiment: \", sentiment_score['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\josh/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\josh\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\josh/nltk_data'\n    - 'C:\\\\Users\\\\josh/nltk_data'\n    - 'C:/Users/josh/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\josh\\Documents\\CST435\\SentimentalAnalysis.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/josh/Documents/CST435/SentimentalAnalysis.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m row\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/josh/Documents/CST435/SentimentalAnalysis.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Apply the sentiment analysis function to each row in the DataFrame\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/josh/Documents/CST435/SentimentalAnalysis.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mapply(analyze_sentiment_for_row, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9557\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9559\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   9560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   9561\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9566\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   9567\u001b[0m )\n\u001b[1;32m-> 9568\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 764\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 891\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    893\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    905\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    906\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 907\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[0;32m    908\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    909\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    910\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    911\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\josh\\Documents\\CST435\\SentimentalAnalysis.ipynb Cell 6\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josh/Documents/CST435/SentimentalAnalysis.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39manalyze_sentiment_for_row\u001b[39m(row):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josh/Documents/CST435/SentimentalAnalysis.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Tokenize and tag the text in the row\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/josh/Documents/CST435/SentimentalAnalysis.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     sentence \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(row[\u001b[39m'\u001b[39;49m\u001b[39mreview_text\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josh/Documents/CST435/SentimentalAnalysis.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     pos_tags \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mpos_tag(sentence)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/josh/Documents/CST435/SentimentalAnalysis.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Perform NER using your custom parser\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\josh/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\josh\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\josh/nltk_data'\n    - 'C:\\\\Users\\\\josh/nltk_data'\n    - 'C:/Users/josh/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\josh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.data.path.append('C:/Users/josh/nltk_data')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
